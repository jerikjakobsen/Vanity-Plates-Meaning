{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**BART Fine Tuning for deriving plate meaning**\n",
        "\n",
        "*Note, the original code was run locally using PyCharm, so I am unsure if this would all work on colab.*"
      ],
      "metadata": {
        "id": "M-36lC7XiPxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "AdKK6AJRj_3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import torch"
      ],
      "metadata": {
        "id": "noLlk4Mxj8jS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up"
      ],
      "metadata": {
        "id": "sQqKx7wfjLmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j6H1xZvfW1t"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"golden_standard.csv\").dropna()\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "\n",
        "# Load the BART model and tokenizer\n",
        "model_name = 'facebook/bart-large'\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name).to('cuda')\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing (getting embeddings)"
      ],
      "metadata": {
        "id": "48nZx2ROjglJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_data(batch):\n",
        "\n",
        "    # buh\n",
        "    input_text = batch['plate']\n",
        "    # buh\n",
        "    #input_text = []\n",
        "    #for plate in batch['plate']:\n",
        "    #    input_text.append(f\"Translate the meaning of the following valid plate to meaning: {plate}\")\n",
        "    # buh\n",
        "\n",
        "    inputs = tokenizer(input_text, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    targets = tokenizer(batch['meaning'], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    batch['input_ids'] = inputs['input_ids']\n",
        "    batch['attention_mask'] = inputs['attention_mask']\n",
        "    batch['labels'] = targets['input_ids']\n",
        "    return batch\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
        "print(train_dataset)\n",
        "print(train_dataset.data)"
      ],
      "metadata": {
        "id": "fq9jH5OMjkk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "VcA_Bkz6jo4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=10,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    report_to=\"none\",\n",
        "    fp16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Done Training\")"
      ],
      "metadata": {
        "id": "_V0n9avdjoET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating Predictions"
      ],
      "metadata": {
        "id": "2bMjvoV-j1aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(dataset):\n",
        "    model.eval()\n",
        "    plates = []\n",
        "    true_meanings = []\n",
        "    predicted_meanings = []\n",
        "\n",
        "    for item in dataset:\n",
        "        input_ids = torch.tensor(item['input_ids']).unsqueeze(0).to(model.device)\n",
        "        attention_mask = torch.tensor(item['attention_mask']).unsqueeze(0).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=128)\n",
        "\n",
        "        plates.append(tokenizer.decode(item['input_ids'], skip_special_tokens=True))\n",
        "        true_meanings.append(tokenizer.decode(item['labels'], skip_special_tokens=True))\n",
        "        predicted_meanings.append(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"plate\": plates,\n",
        "        \"true_meaning\": true_meanings,\n",
        "        \"predicted_meaning\": predicted_meanings\n",
        "    })\n",
        "\n",
        "predictions_df = generate_predictions(test_dataset)\n",
        "\n",
        "print(\"Done Predicting\")\n",
        "predictions_df.to_csv(\"test_predictions.csv\", index=False)"
      ],
      "metadata": {
        "id": "tAqiZ_VTj5Zu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}