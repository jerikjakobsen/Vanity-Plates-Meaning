{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flash_attn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mflash_attn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flash_attn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encountered exception while importing einops: No module named 'einops'\n",
      "Encountered exception while importing mamba_ssm: No module named 'mamba_ssm'\n",
      "Encountered exception while importing causal_conv1d: No module named 'causal_conv1d'\n",
      "Encountered exception while importing flash_attn: No module named 'flash_attn'\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "This modeling file requires the following packages that were not found in your environment: einops, mamba_ssm, causal_conv1d, flash_attn. Run `pip install einops mamba_ssm causal_conv1d flash_attn`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnvidia/Hymba-1.5B-Base\u001b[39m\u001b[38;5;124m\"\u001b[39m, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:553\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_remote_code \u001b[38;5;129;01mand\u001b[39;00m trust_remote_code:\n\u001b[0;32m    552\u001b[0m     class_ref \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mauto_map[\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m]\n\u001b[1;32m--> 553\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m get_class_from_dynamic_module(\n\u001b[0;32m    554\u001b[0m         class_ref, pretrained_model_name_or_path, code_revision\u001b[38;5;241m=\u001b[39mcode_revision, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    555\u001b[0m     )\n\u001b[0;32m    556\u001b[0m     _ \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:540\u001b[0m, in \u001b[0;36mget_class_from_dynamic_module\u001b[1;34m(class_reference, pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, code_revision, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m     code_revision \u001b[38;5;241m=\u001b[39m revision\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# And lastly we get the class inside our newly created module\u001b[39;00m\n\u001b[1;32m--> 540\u001b[0m final_module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[0;32m    541\u001b[0m     repo_id,\n\u001b[0;32m    542\u001b[0m     module_file \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    543\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    544\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    545\u001b[0m     resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[0;32m    546\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    547\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    548\u001b[0m     revision\u001b[38;5;241m=\u001b[39mcode_revision,\n\u001b[0;32m    549\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    550\u001b[0m     repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m    551\u001b[0m )\n\u001b[0;32m    552\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_class_in_module(class_name, final_module, force_reload\u001b[38;5;241m=\u001b[39mforce_download)\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:365\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[1;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;66;03m# Check we have all the requirements in our environment\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m modules_needed \u001b[38;5;241m=\u001b[39m check_imports(resolved_module_file)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# Now we move the module inside our cached dynamic modules.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m full_submodule \u001b[38;5;241m=\u001b[39m TRANSFORMERS_DYNAMIC_MODULE_NAME \u001b[38;5;241m+\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msep \u001b[38;5;241m+\u001b[39m submodule\n",
      "File \u001b[1;32mc:\\Users\\John\\anaconda3\\Lib\\site-packages\\transformers\\dynamic_module_utils.py:197\u001b[0m, in \u001b[0;36mcheck_imports\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_packages) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 197\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis modeling file requires the following packages that were not found in your environment: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Run `pip install \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_packages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_relative_imports(filename)\n",
      "\u001b[1;31mImportError\u001b[0m: This modeling file requires the following packages that were not found in your environment: einops, mamba_ssm, causal_conv1d, flash_attn. Run `pip install einops mamba_ssm causal_conv1d flash_attn`"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"nvidia/Hymba-1.5B-Base\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - defaults\n",
      " - nvidia\n",
      " - pytorch\n",
      "Platform: win-64\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - mamba_ssm\n",
      "  - flash_attn\n",
      "  - einops\n",
      "  - causal_conv1d\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main\n",
      "  - https://repo.anaconda.com/pkgs/r\n",
      "  - https://repo.anaconda.com/pkgs/msys2\n",
      "  - https://conda.anaconda.org/nvidia\n",
      "  - https://conda.anaconda.org/pytorch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install flash_attn mamba_ssm einops causal_conv1d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
